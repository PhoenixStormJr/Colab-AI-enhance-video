{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhoenixStormJr/Unfinished-Colab-AI-enhance-video/blob/main/whisper_upload_translated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to Google Drive and upload the file to the root of the drive\n",
        "\n",
        "https://drive.google.com/drive/"
      ],
      "metadata": {
        "id": "R7OdheAWtwnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Video file name** { display-mode: \"form\" }\n",
        "file_name = input(\"Enter file name: \") or \"input.mp3\"\n",
        "file_name_audio = str(file_name)"
      ],
      "metadata": {
        "id": "OBseFR5gkV7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WYSV-73rslm"
      },
      "outputs": [],
      "source": [
        "#@title ##**Install dependencies** { display-mode: \"form\" }\n",
        "!git clone https://huggingface.co/spaces/openai/whisper\n",
        "%cd whisper\n",
        "!pip install -r requirements.txt\n",
        "!pip install gradio\n",
        "!pip install pysrt\n",
        "import pysrt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Connecting to Google Drive** { display-mode: \"form\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/mnt/gdrive')\n"
      ],
      "metadata": {
        "id": "eZHXI6Ukkfbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Split file** { display-mode: \"form\" }\n",
        "from pydub import AudioSegment\n",
        "from pydub.utils import make_chunks\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "split = \"auto\" #@param [\"30_second\",\"auto\"]\n",
        "chunk_folder = \"/content/whisper/audio_chunks\"\n",
        "if os.path.isdir(chunk_folder):\n",
        "    shutil.rmtree(chunk_folder)\n",
        "\n",
        "if (split == \"30_second\"):\n",
        "\n",
        "  path = '/mnt/gdrive/MyDrive/'\n",
        "  full_path = os.path.join(path, file_name)\n",
        "\n",
        "  def split_audio_file(audio_file_path, chunk_length_ms=30000):\n",
        "\n",
        "      audio = AudioSegment.from_file(audio_file_path)\n",
        "      audio_chunks = make_chunks(audio, chunk_length_ms)\n",
        "\n",
        "      os.makedirs(chunk_folder, exist_ok=True)\n",
        "\n",
        "      for i, chunk in enumerate(audio_chunks):\n",
        "          chunk_name = f'{chunk_folder}/chunk_{i:04d}.wav'\n",
        "          print(chunk_name)\n",
        "          chunk.export(chunk_name, format=\"wav\")\n",
        "\n",
        "      return chunk_folder\n",
        "\n",
        "  split_audio_file(full_path)\n",
        "\n",
        "if (split == \"auto\"):\n",
        "  from librosa.util.utils import normalize\n",
        "  import os\n",
        "  import librosa\n",
        "  import numpy as np\n",
        "  import soundfile as sf\n",
        "  import shutil\n",
        "  import audioread\n",
        "\n",
        "  import scipy.io.wavfile as wav\n",
        "  import scipy.signal as signal\n",
        "  from scipy.signal import find_peaks\n",
        "  from scipy.io import wavfile\n",
        "  import moviepy.editor as mp\n",
        "  from scipy.io import loadmat\n",
        "\n",
        "\n",
        "  chunk_folder = \"/content/whisper/audio_chunks\"\n",
        "  if os.path.isdir(chunk_folder):\n",
        "      shutil.rmtree(chunk_folder)\n",
        "  os.makedirs(chunk_folder)\n",
        "  path = '/mnt/gdrive/MyDrive/'\n",
        "\n",
        "  full_path = os.path.join(path, file_name)\n",
        "\n",
        "  def convert_to_wav(input_file):\n",
        "      file_name, ext = os.path.splitext(input_file)\n",
        "      if ext == '.wav':\n",
        "          return input_file\n",
        "\n",
        "      output_file = file_name + '.wav'\n",
        "      sound = AudioSegment.from_file(input_file, format=ext[1:])\n",
        "      sound.export(output_file, format='wav')\n",
        "      return output_file\n",
        "\n",
        "  full_path = convert_to_wav(full_path)\n",
        "  print(full_path)\n",
        "  # Upload an audio file and get signal data\n",
        "  rate, data = wavfile.read(full_path)\n",
        "\n",
        "  # Normalize audio to -3 dB\n",
        "  target_db = -3.0\n",
        "  max_val = np.iinfo(data.dtype).max\n",
        "  target_val = np.power(10.0, target_db / 20.0) * max_val\n",
        "  data = data.astype(np.float64) * target_val / np.max(np.abs(data))\n",
        "  data = data.astype(np.int16)\n",
        "\n",
        "  # Save normalized audio file\n",
        "  normalize_file = f\"/mnt/gdrive/MyDrive/normalized_{file_name}\"\n",
        "  wavfile.write(normalize_file, rate, data)\n",
        "\n",
        "  def split_chunk(input_file, output_dir, intervals_ms):\n",
        "      # Open an audio file and load its content and sample rate\n",
        "      audio, sr = sf.read(input_file)\n",
        "\n",
        "      # Split an audio file into separate parts\n",
        "      for i in range(len(intervals_ms) - 1):\n",
        "          start_ms = intervals_ms[i]\n",
        "          end_ms = intervals_ms[i+1]\n",
        "\n",
        "          # Calculate start and end indexes in samples\n",
        "          start = int(start_ms / 1000 * sr)\n",
        "          end = int(end_ms / 1000 * sr)\n",
        "\n",
        "          # Write a separate part of an audio file to a new file\n",
        "          output_file = f'{chunk_folder}/chunk_{i:04d}.wav'\n",
        "          sf.write(output_file, audio[start:end], sr)\n",
        "\n",
        "\n",
        "  def split_audio(input_file, output_dir, min_duration=1.0, max_duration=10.0, silence_threshold=0.5, min_length=5):\n",
        "      # Upload an audio file and get signal data\n",
        "      audio_file, sr = librosa.load(input_file, sr=None, mono=True)\n",
        "\n",
        "      # Calculate frame duration and frame size\n",
        "      frame_duration = 0.025\n",
        "      frame_len = int(frame_duration * sr)\n",
        "\n",
        "      # Calculate the threshold value of the energy signal to determine silence\n",
        "      peak_threshold = np.max(audio_file) * silence_threshold\n",
        "\n",
        "      # Calculate energy signal\n",
        "      audio_file = np.concatenate((audio_file, np.zeros(frame_len - len(audio_file) % frame_len)))\n",
        "      energies = np.sum(np.square(audio_file.reshape(-1, frame_len)), axis=1)\n",
        "\n",
        "      # Find peaks in energy signal\n",
        "      peak_indices, _ = find_peaks(energies, height=peak_threshold)\n",
        "\n",
        "      # Create an array of silence intervals\n",
        "      silence_intervals = []\n",
        "\n",
        "      # If the first peak is not at the beginning of the signal, add an interval from the beginning to the first peak\n",
        "      if peak_indices[0] != 0:\n",
        "          silence_intervals.append((0, peak_indices[0] * frame_len))\n",
        "\n",
        "      # Add silence intervals between peaks if the interval length is greater than min_duration\n",
        "      for i in range(len(peak_indices) - 1):\n",
        "          start = peak_indices[i] * frame_len\n",
        "          end = peak_indices[i+1] * frame_len\n",
        "          duration = (end - start) / sr\n",
        "          if duration > min_duration:\n",
        "              silence_intervals.append((start, end))\n",
        "\n",
        "      # Add the last silence interval if its length is greater than min_duration and less than or equal to max_duration\n",
        "      if peak_indices[-1] * frame_len != len(audio_file) - 1:\n",
        "          start = peak_indices[-1] * frame_len\n",
        "          end = len(audio_file) - 1\n",
        "          duration = (end - start) / sr\n",
        "          if duration > min_duration and duration <= max_duration:\n",
        "              silence_intervals.append((start, end))\n",
        "\n",
        "      silence_intervals = [(start * 1000 / sr, end * 1000 / sr) for start, end in silence_intervals]\n",
        "\n",
        "      means = []\n",
        "      for start, end in silence_intervals:\n",
        "          mean = (start + end) / 2\n",
        "          means.append(mean)\n",
        "      # calculate the length of the audio file in milliseconds and add it to the end of the intervals_ms list\n",
        "      file_duration_ms = np.round(len(audio_file) / sr * 1000).astype(int)\n",
        "      means = np.append(means, file_duration_ms)\n",
        "      means = np.insert(means, 0, 0)\n",
        "      means = means.astype(int)\n",
        "      i = 0\n",
        "      min_interval = 15000\n",
        "      max_interval = 30000\n",
        "      while i < len(means) - 1:\n",
        "          # check if next element is less than current element + 15\n",
        "          if means[i+1] < means[i] + min_interval:\n",
        "              #@markdown remove next element\n",
        "              means = np.delete(means, i+1)\n",
        "          # check if next element is equal to current element + 15 and less than current element + 30\n",
        "          elif means[i+1] == means[i] + min_interval and means[i+1] < means[i] + max_interval:\n",
        "              #@markdown leave the next element and move on to the next iteration\n",
        "              i += 1\n",
        "          # check if next element is greater than current element + 30\n",
        "          elif means[i+1] > means[i] + max_interval:\n",
        "              # add a new element that will be equal to the current one + 30 and move on to the next iteration\n",
        "              means = np.insert(means, i+1, means[i]+max_interval)\n",
        "              i += 1\n",
        "          else:\n",
        "              i += 1\n",
        "      means = np.append(means, file_duration_ms)\n",
        "      print(silence_intervals)\n",
        "      print(means)\n",
        "      split_chunk(full_path, chunk_folder, means)\n",
        "      return means\n",
        "\n",
        "  time_labels = split_audio(normalize_file, chunk_folder)"
      ],
      "metadata": {
        "id": "xgBqiVmgk3Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Model installation** { display-mode: \"form\" }\n",
        "import os\n",
        "os.system(\"pip install git+https://github.com/openai/whisper.git\")\n",
        "import gradio as gr\n",
        "import whisper\n",
        "\n",
        "from share_btn import community_icon_html, loading_icon_html, share_js\n",
        "\n",
        "model = whisper.load_model(\"large-v2\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dMhWUqKdNzn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Launch** { display-mode: \"form\" }\n",
        "import os\n",
        "\n",
        "import pysrt\n",
        "import datetime\n",
        "\n",
        "result_subs = pysrt.SubRipFile()\n",
        "\n",
        "import datetime\n",
        "import pysrt\n",
        "\n",
        "def create_subtitles(start_time, end_time, text):\n",
        "    subs = pysrt.SubRipFile()\n",
        "    start_time = datetime.timedelta(milliseconds=int(start_time))\n",
        "    end_time = datetime.timedelta(milliseconds=int(end_time))\n",
        "    start_time_dict = {'hours': 0, 'minutes': 0, 'seconds': start_time.seconds, 'milliseconds': start_time.microseconds // 1000}\n",
        "    end_time_dict = {'hours': 0, 'minutes': 0, 'seconds': end_time.seconds, 'milliseconds': end_time.microseconds // 1000}\n",
        "    subs.append(pysrt.SubRipItem(index=1, start=pysrt.SubRipTime(**start_time_dict), end=pysrt.SubRipTime(**end_time_dict), text=text))\n",
        "    return subs\n",
        "\n",
        "\n",
        "\n",
        "def inference(audio):\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    options = whisper.DecodingOptions(fp16 = False)\n",
        "    result = whisper.decode(model, mel, options)\n",
        "\n",
        "    print(result.text)\n",
        "    return result.text, gr.update(visible=True), gr.update(visible=True), gr.update(visible=True)\n",
        "\n",
        "# Processing each piece and writing the result to a file\n",
        "result_text = \"\"\n",
        "chunk_folder = \"/content/whisper/audio_chunks\"\n",
        "chunk_files = sorted(os.listdir(chunk_folder))\n",
        "for i in range(len(chunk_files)-1):\n",
        "    file_name = chunk_files[i]\n",
        "    if file_name.endswith('.wav'):\n",
        "        file_path = os.path.join(chunk_folder, file_name)\n",
        "        text = inference(file_path)[0]\n",
        "        result_text += text\n",
        "\n",
        "        start_time = time_labels[i]\n",
        "        end_time = time_labels[i+1] if i+1<len(time_labels) else start_time + 5000\n",
        "        subs = create_subtitles(start_time, end_time, text)\n",
        "        result_subs.extend(subs)\n",
        "\n",
        "# Saving the result to a file\n",
        "with open('result.txt', 'w') as f:\n",
        "    f.write(result_text)\n",
        "name_without_extension = os.path.splitext(file_name_audio)[0]\n",
        "path_str = \"/content/whisper/\"+name_without_extension+\".srt\"\n",
        "result_subs.save(path_str)"
      ],
      "metadata": {
        "id": "lQuY8YCTXgtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Download result** { display-mode: \"form\" }\n",
        "import re\n",
        "\n",
        "with open('result.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# replace the end of the sentence with a line break\n",
        "text = re.sub(r'([.?!])\\s+', r'\\1\\n', text)\n",
        "\n",
        "with open('result.txt', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# loading result.txt file into Colab\n",
        "files.download('result.txt')"
      ],
      "metadata": {
        "id": "W5KkrfYpog_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Download subtitles** { display-mode: \"form\" }\n",
        "from google.colab import files\n",
        "files.download(path_str)"
      ],
      "metadata": {
        "id": "ck7y_l7DPAby"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}